# -*- coding: utf-8 -*-
"""boston_housing_examp02..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bNXnksp7Jxi5xhLhSxIY4vCipQL2pYbN
"""

#1. 데이터 수집
import tensorflow as tf
(x_train, y_train), (x_test, y_test) =tf.keras.datasets.boston_housing.load_data(
    path='boston_housing.npz', test_split=0.2, seed=113
)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#2. 데이터의 유형이나 관련 필드 등을 분석 분류

#도시별 1인당 범죄율 CRIM :25,000제곱피트(sq.ft) 이상 부지로 지정된 주거용 토지의 비율 ZN
#INDUS: 도시별 비소매업용 면적 비율
#CHAS: 찰스 강 더미 변수 (= 해당 구역이 강 경계를 포함하는 경우 1, 그렇지 않은 경우 0)
#NOX: 산화질소 농도 (1,000만 분의 1)
#RM: 가구당 평균 방 개수
#AGE: 1940년 이전에 지어진 자가 주택의 비율
#DIS: 보스턴 5개 고용 센터까지의 가중 거리
#RAD: 방사형 고속도로 접근성 지수
#TAX: 1만 달러당 재산세 전액 세율
#PTRATIO: 도시별 교사-학생 비율
#B 1000(Bk - 0.63)^2 (Bk는 도시별 흑인 비율)
#LSTAT: 인구 하위 계층 비율 (%)
#정답 MEDV: 자가 주택의 중간 가치 (1,000달러대)

#데이터 유형검사
for ix,d in enumerate(x_train[0]):
  print(ix+1, "타입 : ", type(d))
print("정답데이터타입:",type(y_train[0]))
data_label = ["범죄율","토지비율","비소매업면적","강인접",
              "질소농도","평균방수","고택율","고용거리","고속접근","세율",
              "교학생비율","흑인비율","하위계층비율"]
for ix, d in enumerate(x_train[0]):
  print(f". {data_label[ix]}: {d}",end="|")
print()
print("평균주택가격:",y_train[0]*1000*1400,"원")

import matplotlib.pyplot as plt
for ix in range(len(x_train[0])):
  plt.subplot(4,4,ix+1)
  plt.scatter(y_train,x_train[:,ix],label=data_label[ix],s=1)
plt.title(ix)

# 데이터 크기
for ix in range(len(x_train[0])):
  print(ix,".")
  print("max",x_train[:,ix].max(),end=":")
  print("min",x_train[:,ix].min(),end=".")
  print("mean",x_train[:,ix].mean(),end=":")
  print("std",x_train[:,ix].std(),end=":")

# 3번과 4번 필드를 제외한 데이터 정규화 스케일링
except_datas= [3,4]
norm_mean = []
norm_std = []
for ix in range(len(x_train[0])):
  if ix in except_datas:
    norm_mean.append(0)
    norm_std.append(0)
    continue
  norm_mean.append(x_train[:,ix].mean())
  norm_std.append(x_train[:,ix].std())
def normal_data(target_data):
  for ix in range(len(target_data[0])):
    if ix in except_datas:
      continue
    #데이터를 표준 정규분포로 스케일링
    target_data[:,ix] = (target_data[:,ix]-norm_mean[ix])/norm_std[ix]
  return target_data

x_train =normal_data(x_train)
x_test  = normal_data(x_test)
print(x_train[:3])
print(x_test[:3])
print(x_train[:,0].std())
print(x_train[:,0].mean())
print(x_train[:-1].std)
print(x_train[:,-1].mean())

#4.모델 생성
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import Dense, Dropout
print(x_train.shape)

import numpy as np
import random
random.seed(123)
np.random.seed(123)
tf.random.set_seed(123)

model = Sequential()
model.add(Input((13,)))
model.add(Dense(256,activation="relu"))
#l(np.array([x_train[0]]))
#res[0]))
#print(res)
model.add(Dropout(0.4))
model.add(Dense(64,activation="relu"))
model.add(Dropout(0.3))
model.add(Dense(16,activation="relu"))
model.add(Dense(1))
model.compile(loss="mse", optimizer="sgd", metrics=['mae'])

print(x_train[:].max())
print(x_test[:].max())
print(x_train[:].min())
print(x_test[:].min())

# 훈련시작
y_mean=y_train.mean()
y_std=y_train.std()
y_train=(y_train-y_mean)/y_std
y_test=(y_test-y_mean)/y_std
fhist= model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=500)

plt.plot(fhist.history["loss"],label="mse")
plt.plot(fhist.history["mae"],label="mae")
plt.legend()
plt.show()

y_pred = model.predict(x_test)
plt.plot(y_test,y_test, label= "y_True")
plt.scatter(y_pred,y_pred,label="y_Pred", c= "red",s=1)
plt.legend
plt.show()
print(y_test[0])
print(y_pred[0])

#오차율 계산
print(y_pred.shape)
print(y_test.shape)
y_pred = y_pred.reshape(-1)
print(y_pred.shape)

# t = (y-mean) /std > y=t*std+mean
#y_mean
#y_std
y_pred = y_pred*y_std+y_mean
y_test = y_test*y_std+y_mean
print(y_pred[0])
print(y_test[0])
print((1-(y_pred[0]/y_test[0]))*100)
rate =1-y_pred/y_test
print(rate[:5])
rate=np.absolute(rate)
print(rate[:5])
err_rate_mean = rate[:5].mean()
print(f"현재 모델의 전체 오차 평균 백분율은 {err_rate_mean:.2%} ")

print(21280000/1000/1400)